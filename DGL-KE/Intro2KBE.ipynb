{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../supporting_examples/')\n",
    "import examples\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a  Knowledge Graph\n",
    "“A knowledge graph (i) mainly describes real world entities and their interrelations, organized\n",
    "in a graph, (ii) defines possible classes and relations of entities in a schema, (iii) allows for\n",
    "potentially interrelating arbitrary entities with each other and (iv) covers various topical\n",
    "domains.” [Paulheim- 1]\n",
    "\n",
    "Knowledge Graphs are often modeled as directed multigraphs. As a quick reminder let us define Multigrpah and directed graphs:\n",
    "- In a directed graph edges are called arcs and they can direct information from their head to their tail but not in the opposite direction.\n",
    "- A multigraph is a graph that can includes loops as well as multiple relationships between the same nodes.\n",
    "for more information on Graphs you can check this [link](https://github.com/cyrusmvahid/GNNTrainingMaterial/blob/master/March2020/01-gentle_into_2_graphs.ipynb) \n",
    "\n",
    "Let us examine a directed multigraph in an example, which include a cast of characters and the world in which they live. \n",
    "\n",
    "**scenario:**\n",
    "\n",
    "**Mary** and **Tom** are ***siblings*** and they both are ***are vegetarians***, who ***like*** **potatoes** and **cheese**. Mary and Tom both ***work*** at **Amazon**. **Joe** is a bloke who is a ***colleague*** of Tom. To make the matter complicated, Joe ***loves*** Mary, but we do not know if the feeling is reciprocated. \n",
    "\n",
    "Joe ***is from*** **Quebec** and is proud of his native dish of **Poutine**, which is ***composed*** of potato, cheese, and **gravy**. We also know that gravy ***contains*** **meat** in some form.\n",
    "\n",
    "Joe is excited to invite Tom for dinner and and has sneakily included his sibling, Mary, in the invitation. His plans are doomed from get go as he is planning to serve the vegetarian siblings his favourite Quebecois dish, Poutine.\n",
    "\n",
    "Oh! by the way, a piece of geography trivia: Quebec ***is located*** in a **province** of the same name, which in turn ***is located*** in **Canada**.\n",
    "\n",
    "**end of scenario**\n",
    "\n",
    "There is several relationships in this scenario that are not explicitly mentioned but we can simply infer from what we are given:\n",
    "- Mary is a colleague of Tom.\n",
    "- Tom is a colleague of Mary.\n",
    "- Mary is Tom's sister.\n",
    "- Tom is Mary's brother.\n",
    "- Poutine has meat.\n",
    "- Poutine is not a vegetarian dish.\n",
    "- Mary and Tom would not eat Poutine.\n",
    "- Poutine is a Canadian dish.\n",
    "- Joe is Canadian.\n",
    "- Amazon is a workplace for Mary, Tom, and Joe.\n",
    "\n",
    "There are also some interesting negative conclusions that seems intuitive to us but not to the machine:\n",
    "- Potato *does not like* Mary.\n",
    "- Canada *is not from* Joe.\n",
    "- Canada *is not located** in Quebec.\n",
    "- ...\n",
    "What we examined is a knowledge graph, a set of nodes with different types of relations:\n",
    "- 1-to-1: Mary is a sibling of Tom.\n",
    "- 1-to-N: Amazon is a workplace for Mary, Tom, and Joe.\n",
    "- N-to-1: Joe, Tom, and Mary work at Amazon.\n",
    "- N-to-N: Joe, Mary, and Tom are colleagues.\n",
    "\n",
    "There are other categorization perspectives on the relationships as well:\n",
    "- Symmetric: Joe is a colleague of Tom entails Tom is also a colleague of Joe.\n",
    "- Antisymmetric: Quebec is located in Canada entails that Canada cannot be located in Quebec.\n",
    "\n",
    "Figure 1: visualized a knowledgebase that describes *World of Mary*. For more information on how to use the examples, please refer to the [code](https://github.com/cyrusmvahid/GNNTrainingMaterial/blob/master/March2020/supportingexamples/examples.py) that draws the examples. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"img/wom_c.png\" height=\"800\" width=\"800\">\n",
    "    <figcaption>Figure1: World of Mary </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the task of Knowledge Graph Embedding?\n",
    "Knowledge graph embedding is the task of completing the knowledge graphs by probabilistically inferring the missing arcs from the existing graph structure. KGE differs from ordinary relation inference as the information in a knowledge graph are multi-relational and more complex to model and computationally expensive. For this rest of this blog, we examine fundamentals of KGE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common connectivity patterns:\n",
    "The connectivity or relational pattern are commonly observed in KG. A Knowledge Graph Embedding model intends to predicts missing connections that are often one of the below types.\n",
    "\n",
    "- ***symmetric***\n",
    " - **Definision:** A relation $r$ is ***symmetric** if $\\forall {x,y}: (x,r,y)\\implies (y,r,x)$\n",
    " - **Example:** $\\text{x=Mary and y=Tom and r=\"is a sibling of\"}; (x,r,y) = \\text{Mary is a sibling of Tom} \\implies (y,r,x)=\\text{Tom is a sibling of Mary}$ \n",
    "\n",
    "- ***antisymmetric***\n",
    " - **Definision:** A relation r is ***antisymmetric** if $\\forall {x,y}: (x,r,y)\\implies \\lnot (y,r,x)$\n",
    " - **Example:** $\\text{x=Quebec and y=Canada and r=\"is located in\"}; (x,r,y) = \\text{Quebec is located in Canada} \\implies (y,\\lnot r,x)=\\text{Canada is not located in Quebec}$ \n",
    "- ***inversion***\n",
    " - **Definition:** A relation $r_1$ is ***inverse*** to relation $r_2$ if $\\forall x,y: r_2(x,y)\\implies r_1(y,x)$.\n",
    " - **Example:** $x=Mary,\\ y=Tom,\\ r_1=\\text{\"is a sister of}\"\\ and r_2=\\text{\"is a brother of\"} \\\\ (x,r_1,y)=\\text{Mary is a sister of Tom} \\implies (y,r_2,x) = \\text{Tom is a brother of Mary}$\n",
    "- ***composition***\n",
    " - **Definition**: relation $r_1$ is composed of relation $r_2$ and relation $r_3$ if $\\forall x,y,z: (x,r_2,y) \\land (y,r_3,z) \\implies (x,r_1, z)$\n",
    " - **Example:** $\\text{x=Tom, y=Quebec, z=Canada},\\ r_2=\\text{\"is born in\"}, r_3=\\text{\"is located in\"}, r_1=\\text{\"is from\"}\\\\(x,r_2,y)=\\text{Tom is born in Quebec} \\land (y,r_3,z) = \\text{Quebec is located in Canada} \\implies (x,r_1,z)=\\text{Tom is from Canada}$\n",
    "\n",
    "*ref: RotateE[2]*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Function\n",
    "There are different flavours of KGE that have been developed over the course of the past few years. What most of them have in common is a score function. The score function measures how distant two nodes in relations to a relation are. As we are setting the stage to introduce the reader to DGL-KE, an open source knowledge graph embedding library, we limit the scope only to those methods that are implemented by DGL-KE and are listed in figure 2. \n",
    "<figure>\n",
    "    <img src=img/scorefunction.png height=200, width=300 align=top>\n",
    "    <figcaption>Figure2: A list of score functions for KE papers implemented by DGL-KE</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A short explanation of the score functions\n",
    "Knowledge graphs that are beyond toy examples are always large, high dimensional, and sparse. High dimensionality and sparsity are the results of the amount of information that the KG holds that can be represented with 1-hot or n-hot vectors. The fact that most of the items have no relationship withe one another is another major contributor to sparsity of KG representations. We, therefore desire to project the sparse and high dimensional graph representational vector space onto a lower dimensional dense space. This process is similar to to word embedding and dimensionality reduction used for [recommender systems baesd on matrix factorization models.](https://www.slideshare.net/apachemxnet/building-content-recommendation-systems-using-mxnet-gluon)\n",
    "I will provide a detailed account of all the methods in a different post, but here I will provide a short explanation of how projections differ in each paper, what the score functions do, and what consequences the choices have on on relationship inference and computational complexity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransE:\n",
    "TransE is a representative translational distance model that represents entities and relations as vectors in the same semantic space of dimension $\\mathbb{R^d}$, where $d$ is the dimension of the target space with reduced dimension. A fact in the source space is represented as a triplet $(h, r, t)$ where $h$ is short for *head*, $r$ is for *relation*, and $t$ is for *tail*. The relationship is interpreted as a translation vector so that the embedded entities are connected by relation $r$ have a short distance. [3, 4]\n",
    "In terms of vector computation it could mean adding a head to a relation should approximate to the relation's tail, or $h+r \\approx t$. For example if $h_1=emb(\"Ottawa\"),\\ h_2=emb(\"Berlin\"), t_1=emb(\"Canada\"), t_2=(\"Germany\")$, and finally $r=\"CapilatOf\"$, then $h_1 + r$ and $h_2+r$ should approximate $t_1$ and $t_2$ respectively.\n",
    "TransE performs linear transformation and the scoring function is negative distance between $h+r$ and $t$, or $f=-\\|h+r-t\\|_{\\frac{1}{2}}$\n",
    "<figure>\n",
    "    <img src=img/transe.png hight=200 width=200>\n",
    "    <figcaption>Figure 3: TransE</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransR\n",
    "TransE cannot cover relationship that are not 1-to-1 as it learns only one aspect of similarity. TransR addresses this issue with separating relationship space from entity space where $h, t \\in \\mathbb{R}^k$ and $r \\in \\mathbb{R}^d$. The semantic spaces do not need to be of the same dimension. In the multi-relationship modeling we learn a projection matrix $M\\in \\mathbb{R}^{k \\times d}$ for each relationship that can project an entity to different relationship semantic spaces. Each of these spaces capture a different aspect of an entity that is related to a distinct relationship. In this case a head node $h$ and a tail node $t$ in relation to relationship $r$ is projected into the relationship space using the learned projection matrix $M_r$ as $h_r=hM_r$ and $t_r=tM_r$ respectively.Figure 5 illustrates this projection.\n",
    "\n",
    "Let us explore this using an example. Mary and Tom are siblings and colleagues. They both are vegetarians. Joe also works for Amazon and is a colleague of Mary and Tom. TransE might end up learning very similar embedding for Mary, Tom, and Joe from the fact that they are colleagues but fail to recognize the (not)sibling relationship.\n",
    "Using TransR, we learn projection matrices: $M_{sib},\\ M_{clg}\\ ,and M_{vgt}$ that perform better on learning relationship like (not)sibling. \n",
    "The score function in TransR is similar to TransE and measures euclidean distance between $h+r$ and $t$, but the distance measure is per relationship space. More formally: $f_r=\\|h_r+r-t_r\\|_2^2$\n",
    "<figure>\n",
    "    <img src=img/transr.png height=200, width=400>\n",
    "    <figcaption>Figure 4: TransR projecting different aspects of an entity to a relationship space.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Another advantage of TransR over TransE is its ability to extract compositional rules. Ability to extract rules has two major benefits: 1) Richer information, and 2) smaller memory space as we can infer some of the rules from others.\n",
    "\n",
    "#### Drawbacks\n",
    "The benefits from more expressive projections in TransR adds to the complexity of the model as well as higher rate of moving data around which has an adverse effect on distributed training. TransE requires $O(d)$ parameters per relation, where $d$ is the dimension of semantic space in TransE which includes both entities and relationships. As TransR projects entities to a relationship space of dimension $k$, it will require $O(kd)$ parameters per relation. depending on the size of k, this could potentially increase the number of parameters drastically. In exploring DGL-KE, we will examine benefits of DGL-KE in making computation of knowledge embedding significantly more efficient.\n",
    "\n",
    "ref: TransR[5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TransE and its variantes such as TransR are generall called *translational distance models* as they translate the entities and relationships and measure distance in the the target semantic spaces. A second category of KE models is called *semantic matching* that includes models such as RESCAL, DistMulti, and ComplEx.These models make use of a similarity-based scoring function.\n",
    "The first of semantic matching models we explore is RESCAL.\n",
    "\n",
    "# RESCAL\n",
    "RESCAL is a **bilinear** model that captures latent semantics of a knowledge graph through associate entities with vectors and represents each relation as a matrix that **models pairwise interaction** between entities.\n",
    "\n",
    "Multiple relations of any order can be represented as tensors. In fact $n-dimensional$ tensors are by definitions representation of multi-dimensional vector spaces. RESCAL, therefore, proposes to capture entities and relationships as multidimensional tensors as illustrated in figure 5.\n",
    "\n",
    "RESCAL uses semantic web's RDF formation where relationships are modeled as $(subject, predicate, object)$. Tensor $\\mathcal{X}$ contains such relationships as $\\mathcal{X}_{ijk}$ between $i$<sup>th</sup> and $j$<sup>th</sup> entities through $k$<sup>th</sup> relation.\n",
    "Value of $\\mathcal{X}_{ijk}$ is determined as:\n",
    "$$\n",
    "\\mathcal{X}_{ijk} =  \n",
    "     \\begin{cases}\n",
    "       1\\  &\\quad\\text{if }(e_i, r_k, e_j)\\text{ holds}\\\\\n",
    "       0\\  &\\quad\\text{if }(e_i, r_k, e_j)\\text{ does not hold}\n",
    "     \\end{cases}\n",
    "$$\n",
    "\n",
    "<figure>\n",
    "    <img src=img/RESCAL1.png height=200 width=200>\n",
    "    <figcaption>Figure 5: RESCAL captures entities and their relations as multi-dimensional tensor</figcaption>\n",
    "</figure>\n",
    "\n",
    "As entity relationship tensors tends to be sparse, the authors of RESCAL, propose a dyadic decomposition to capture the inherent structure of the relations in the form of a latent vector representation of the entities and an asymmetric square matrix that captures the relationships. More formally each slice of $\\mathcal{X}_k$ is decomposed as a rank$-r$ factorization:\n",
    "$$\n",
    "\\mathcal{X}_k \\approx AR_k\\mathbf{A}^\\top, \\text{ for } k=1, \\dots, m\n",
    "$$\n",
    "where A is an $n\\times r$ matrix of latent-component representation of entities and asymmetrical $r\\times r$ square matrix $R_k$ that models interaction for $k_th$ predicate component in $\\mathcal{X}$.\n",
    "To make sense of it all, let's take a look at an example:\n",
    "\n",
    "$\n",
    "Entities=\\{\\text{Mary :}0, \\text{Tom :}1, \\text{Joe :}2\\} \\\\\n",
    "Relationships=\\{\\text{sibling, colleague}\\} \\\\\n",
    "Relation_{k=0}^{sibling}: \\text{Mary and Tom are siblings but Joe is not their sibling.} \\\\\n",
    "Relations_{k=1}^{colleague}: \\text{Mary,Tom, and Joe are colleagues}\\\\\n",
    "\\text{relationship matrices will model: }\\mathcal{X_k}=\n",
    "\\begin{bmatrix}\n",
    "Mary & Tom  & Joe \\\\\n",
    "Tom  & Joe & Mary \\\\\n",
    "Joe  & Mary  & Tom\n",
    "\\end{bmatrix}\\\\\n",
    "{\\mathcal{X}}_{0:sibling}=\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 0\\\\\n",
    "0 & 0 & 1\\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}\\\\\n",
    "\\mathcal{X}_{1:colleague}=\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 1\\\\\n",
    "1 & 0 & 1\\\\\n",
    "1 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Note that even in such a small knowledge graph where two of the three entities have even a symmetrical relationship, matrix $\\mathcal{X}_k$ is sparse and asymmetrical. Obviously colleague relationship in this example is not representative of a real world problem. Even though such relationships can be created they contain no information as probability of occurring is hight. For instance if we are creating a knowledge graph for for registered members of a website is a specific country, we do not model relations like \"is countryman of\" as it contains little information and has very low entropy.   \n",
    "\n",
    "Next step in RESCAL is decomposing matrices $\\mathcal{X}_k$ using a rank_k decomposition as illustrated in figure 6.\n",
    "<figure>\n",
    "    <img src=img/RESCAL2.png height=200 width=400>\n",
    "    <figcaption>Figure 6: Each of the $k$ slices of martix $\\mathcal{X}$ is factorized to its k-rank components in form of a $n\\times r$ entity-latent component and an asymmetric $r\\times r$ that specifies interactions of entity-latent components per relation.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Now that the structural decomposition of entities and their relationships are modeled we need to create a score function that can predict existence of relationship for those entities we lack their mutual connection information.\n",
    "\n",
    "The score function $f_r(h,t)$ for $h,t\\in \\mathbb{R}^d$, where $h$ and $t$ are representation of *head* and *tail* entities, captures pairwise interactions between entities in $h$ and $t$ through relationship matrix $M_r$ that is the collection of all individual $R_k$ matrices and is of dimension $d\\times d$.\n",
    "$$\n",
    "f_r(h, t) = \\mathbf{h}^\\top M_rt = \\sum_{i=0}^{d-1}\\sum_{j=0}^{d-1}[M_r]_{ij}.h_i.t_j\n",
    "$$\n",
    "\n",
    "Score function $f$ requires $O(d^2)$ parameters per relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1.  http://semantic-web-journal.net/system/files/swj1167.pdf\n",
    "2.  Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. RotatE: Knowledge graph embedding by relational rotation in complex space. CoRR, abs/1902.10197, 2019.\n",
    "3. Knowledge Graph Embedding: A Survey of Approaches and Applications Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. DOI 10.1109/TKDE.2017.2754499, IEEE Transactions on Knowledge and Data Engineering\n",
    "4. transE\n",
    "5.TransR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>y - (y<sup>2</sup> - k) / 2y</code>\n",
    "\n",
    "i<sup>2</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
